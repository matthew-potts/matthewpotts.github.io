{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8436dcd",
   "metadata": {},
   "source": [
    "#### Using LLMs To Analyse the Effect of U.S. Macroeconomic News: An Application to the Global Financial Cycle\n",
    "<small>*10 October 2025*</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c7cda4",
   "metadata": {},
   "source": [
    "In 2013, Helene Rey [identified the Global Financial Cycle](https://www.nber.org/papers/w21162) ('GFCy') to describe the observed co-movement of global capital flows, risky asset prices, and credit growth.\n",
    "\n",
    "'Risk on' periods are associated with increases in the above variables, and subsequent 'risk off' periods are associated with their retrenchment, often resulting in an economic crisis in the afflicted country. Importantly, this co-movement is observed across a spectrum of relevant variables across the globe.\n",
    "\n",
    "What drives this cycle? Rey subsequently [argued the source of the GFCy](https://academic.oup.com/restud/article/87/6/2754/5834728) to be U.S. monetary policy shocks. The 'single global factor' underlying the cycle is driven by changes in U.S. monetary policy, and therefore these changes drive the cycle.\n",
    "\n",
    "The implication of this that instead of their existing a 'trilemma' where countries are forced to choose two out of monetary policy indepedence, an open capital account, and a fixed exchange rate, [there is instead a 'dilemma'](https://www.nber.org/papers/w21162): in the absence of capital controls, a country cannot have monetary sovereignty, regardless of the exchange rate regime.\n",
    "\n",
    "Economists Boehm and Kroner have [added further nuance to this story](https://blogs.lse.ac.uk/usappblog/2025/06/25/how-news-about-the-us-economy-drives-global-financial-conditions/) by showing that it is not just U.S. monetary policy that drives the GFCy, but actually merely *news* about U.S. macroeconomic conditions can explain the co-movement.\n",
    "\n",
    "Negative surprises regarding the U.S. economy increases investor risk-aversion. The GFCy occurs because this increase in risk-aversion occurs globally. The logic here is similar to the ['gamma' model of Gabai & Maggiori](https://academic.oup.com/qje/article-abstract/130/3/1369/1933306), which places investor risk-aversion as the central force in driving global capital flows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a448ccd",
   "metadata": {},
   "source": [
    "#### Using LLMs to Assess Macroeconomic News Coverage\n",
    "\n",
    "Boehm and Kroner's approach uses (surprises in) U.S. statistical releases (e.g. nonfarm payrolls) as the explanatory variable in their estimates of the effect of news on the GFCy; but the focus on 'news' gives the opportunity to experiment by integrating textual data into this domain of macroeconomic analysis, an approach already being [explored in political science](https://kenbenoit.net/pdfs/Benoit_etal_2025_AJPS.pdf).\n",
    "\n",
    "In particular, I'm interested applying LLM assessments of macroeconomic news coverage to analyse the GFCy. If we 'prompt' an LLM to put itself in the position of an international investor reading the latest macroeconomic news releases, then, in seeing the assessment that this LLM makes, we might be able to obtain an estimate a good proxy for investor risk-aversion. Importantly, this would happen in a replicable and scalable way.\n",
    "\n",
    "My goal is to estimate the following equation:\n",
    "$$\n",
    "\\Delta q_{i,t} = \\alpha_{i} + \\beta_{i} \\cdot \\gamma_{i,t} + \\epsilon_{i,t}\n",
    "$$\n",
    "\n",
    "Where $\\Delta q_{i,t}$ represents the change in asset prices in country $i$ at period $t$, and $\\gamma_{i,t}$ LLM rating of story $i$ at time $t$. $\\beta$ can therefore be interpreted as the sensitivity of asset price changes to the LLM rating, and a positive and statstically significant $\\beta$ can be interpreted as evidence for investor risk-aversion affecting global asset prices (and hence the GFCy).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf0df65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5238ab3d",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "source": [
    "####  Code: Retrieving Input Data and Setting The AI Prompts\n",
    "\n",
    "As a baseline analysis, I retrieve 'important' news stories involving the U.S. Federal Reserve from the [Refinitiv News API](https://developers.lseg.com/content/dam/devportal/api-families/refinitiv-data-platform/refinitiv-data-platform-apis/documentation/rdp_news_user_guide.pdf), including the bodies of each news article, the 'sentiment for each will be assessed by the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5238ab3d",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from lseg.data.content import news, historical_pricing\n",
    "import pandas as pd\n",
    "\n",
    "start_date = \"SET-ME\"\n",
    "end_date = \"SET-ME\"\n",
    "articles_count = \"SET-ME\"\n",
    "\n",
    "response = news.headlines.Definition(\n",
    "    query=\"Fed\",\n",
    "    date_from=start_date, \n",
    "    date_to=end_date, \n",
    "    count=articles_count\n",
    ").get_data()\n",
    "fed = response.data.df\n",
    "\n",
    "topnews = news.headlines.Definition(\n",
    "    query=\"TOPNWS\",\n",
    "    date_from=start_date, \n",
    "    date_to=end_date, \n",
    "    count=articles_count\n",
    ").get_data().data.df\n",
    "\n",
    "df_stories = pd.merge(fed, topnews, on=\"storyId\")\n",
    "df_stories = df_stories[df_stories['headline_x'].str.isupper()]\n",
    "\n",
    "timestamps = []\n",
    "bodies = []\n",
    "for story_id in df_stories['storyId']:\n",
    "    response = news.story.Definition(story_id=story_id).get_data()\n",
    "    timestamps.append(response.data.raw['newsItem']['itemMeta']['versionCreated']['$'])\n",
    "    bodies.append(response.data.raw['newsItem']['contentSet']['inlineData'][0]['$'])\n",
    "df_stories['timestamp'] = timestamps\n",
    "df_stories['body'] = bodies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d389c80a",
   "metadata": {},
   "source": [
    "Using the historical pricing data API I retrieve data on a variety of global asset prices.\n",
    "\n",
    "| Country           | Index   |\n",
    "|-------------------|-----------|\n",
    "| Argentina         | .MERV     |\n",
    "| Austria           | .ATX      |\n",
    "| Belgium           | .BFX      |\n",
    "| Brazil            | .BVSP     |\n",
    "| Canada            | .GSPTSE   |\n",
    "| Switzerland       | .SSMI     |\n",
    "| Chile             | .SPIPSA   |\n",
    "| Czech Republic    | .PX       |\n",
    "| Germany           | .GDAXI    |\n",
    "| Denmark           | .OMXC20   |\n",
    "| Spain             | .IBEX     |\n",
    "| Finland           | .OMXHPI   |\n",
    "| France            | .FCHI     |\n",
    "| United Kingdom    | .FTSE     |\n",
    "| United States     | .NDX      |\n",
    "| Japan             | .TOPX     |\n",
    "\n",
    "Because the GFCy is characterised by fast-moving financial variables, resulting in difficulties with identification, I will employ a high-frequency event study. Specifically, I examine the response of asset prices to (the LLM assessment of) news stories about the Federal Reserve, within a configurable window, starting with 30 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce152b1",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "start_times = []\n",
    "for timestamp in df_stories['timestamp']:\n",
    "    timestamp = pd.to_datetime(timestamp)\n",
    "    for index, row in df_indices.iterrows():\n",
    "        start_times.append({'Country': row['Country'], 'Index': row['Indices'], 'start': timestamp, 'Data': None})\n",
    "df_prices = pd.DataFrame(start_times)\n",
    "\n",
    "data = []\n",
    "for index, row in df_prices.iterrows():\n",
    "    timestamp = pd.to_datetime(row['start'])\n",
    "    try:\n",
    "        definition = historical_pricing.summaries.Definition(\n",
    "            row['Index'],     \n",
    "            interval = historical_pricing.Intervals.FIVE_MINUTES,\n",
    "            start = timestamp,\n",
    "            end = timestamp + timedelta(hours = 6))\n",
    "        response = definition.get_data()\n",
    "        data.append(response.data.df[['TRDPRC_1']])\n",
    "    except Exception:\n",
    "        data.append(None)\n",
    "\n",
    "df_prices['Data'] = data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b9e424",
   "metadata": {},
   "source": [
    "#### LLM Choice and Software Environment\n",
    "\n",
    "For the initial analysis I use [OpenAI's GPT-4o model](https://openai.com/index/hello-gpt-4o/), although this can be substituted for other proprietary models to ensure that results are robust across different models. Because emphasis here is placed on powerful, scalable text-analysis, I favour proprietary models, although for scientific reproducibility an 'open-weight' model such as [Deepseek-V3](https://github.com/deepseek-ai/DeepSeek-V3) would allow for greater transparency and control.\n",
    "\n",
    "To proxy for investor global risk-aversion, I ask the LLM to put itself in the shoes of an economist/investment professional reading a news story related to the U.S. macroeconomy, ranking its perceived implications for the story on a scale of 1 (\"Extremely negative for global asset prices\") to 7 (\"Extremely positive for global asset prices.\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d957e14f",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "\t    api_key=\"API_SECRET_KEY\"\n",
    "    )\n",
    "\n",
    "model = \"gpt-4o\"\n",
    "with open(\"prompt.txt\", \"r\") as file:\n",
    "    instructions = file.read()\n",
    "\n",
    "def generate_ratings(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    ratings = []  # Temporary list to store ratings\n",
    "    for _, row in df.iterrows():\n",
    "        response = client.responses.create(\n",
    "            model=model,\n",
    "            instructions=instructions,\n",
    "            input=row['body']\n",
    "        )\n",
    "        ratings.append(response.output_text)  # Append to the temporary list\n",
    "    # Assign the ratings list to the DataFrame after the loop\n",
    "    df['rating'] = ratings\n",
    "    return df\n",
    "\n",
    "ratings = generate_ratings(df_stories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e4fb52",
   "metadata": {},
   "source": [
    "#### Analysis and Results\n",
    "\n",
    "... in the figure below. The coefficient on LLM assessment of news story is statistically significant at the 10% level in each sample, suggesting that ... .\n",
    "\n",
    "|   Sample      | Coefficient | Standard Error | p-value |\n",
    "|---------------|-------------|----------------|---------|\n",
    "|      Full     |      382.3       |         195.3       |    0.051     |\n",
    "|    Developed  |      XXX       |       XXX         |    XXX     |\n",
    "|   Developing  |      XXX       |       XXX         |    XXX     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3ca32e",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "expanded_rows = []\n",
    "\n",
    "for _, row in df_prices.iterrows():\n",
    "    if isinstance(row['Data'], pd.DataFrame): \n",
    "        row['Data']['Timestamp'] = row['Data'].index\n",
    "        data_df = row['Data'].copy()\n",
    "        data_df['start'] = row['start']\n",
    "        data_df['Index'] = row['Index']\n",
    "        expanded_rows.append(data_df)\n",
    "\n",
    "df_prices = pd.concat(expanded_rows, ignore_index=True)\n",
    "df_prices['Change_TRDPRC_1'] = df_prices.groupby(['Index', 'start'])['TRDPRC_1'].shift(6) - df_prices['TRDPRC_1']\n",
    "df_prices['Shifted_Change_TRDPRC_1'] = df_prices.groupby(['Index', 'start'])['Change_TRDPRC_1'].shift(-6)\n",
    "\n",
    "data_for_regression = pd.merge(df_prices, ratings, left_on='start', right_on='timestamp', how='inner')\n",
    "\n",
    "data_for_regression['Timestamp'] = data_for_regression['Timestamp'].dt.tz_localize('UTC')\n",
    "data_for_regression = data_for_regression[data_for_regression['timestamp'] + pd.Timedelta(minutes=30) == data_for_regression['Timestamp']].dropna().reset_index(drop=True)\n",
    "\n",
    "X = pd.to_numeric(data_for_regression['rating'], errors='coerce')\n",
    "y = pd.to_numeric(data_for_regression['Shifted_Change_TRDPRC_1'], errors='coerce')\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "model = sm.OLS(y, X).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0a02b0",
   "metadata": {},
   "source": [
    "These initial results are promising, but I'd like to return to this analysis to ensure robustness across prompts and models. It's not particularly efficient to feed an LLM an entire news story, and so there one might integrate an 'intermediate' stage of text-summarisation before the actual categorisation is done, especially since LLMs have been shown to be [vulnerable to attention drift](https://arxiv.org/abs/2307.03172).\n",
    "\n",
    "Given the computational requirements of extending this initial analysis, costs would soon add-up. I'm therefore interested in experimenting with [the Nebius platform](https://studio.nebius.com) for processing cost-effective batch jobs across a range of LLMs, including open-weight.\n",
    "\n",
    "Until next time,\n",
    "\n",
    "Matt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
